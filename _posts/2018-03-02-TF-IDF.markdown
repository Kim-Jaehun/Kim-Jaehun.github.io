---
layout: post
title:  "TF-IDF"
date:   2018-03-02 08:43:59
author: kim-jaehun
categories:
  -NLP
tags: tf-idf
---

## TF-IDF란?

### Term Frequency - Inverse Document Frequency

* TF(단어 빈도, term frequency)는 특정한 단어가 문서 내에 얼마나 자주 등장하는지를 나타내는 값  
<br/>
* DF(문서 빈도, document frequency)라고 하며, 이 값의 역수를 IDF(역문서 빈도, inverse document frequency)
<br/>
* TF-IDF는 TF와 IDF를 곱한 값

$$\text{tf-idf}(d, t) = \text{tf}(d, t) \cdot \text{idf}(t)$$

<br/><br/>
### 값을 산출하는 방식에는 여러 가지가 있다

TF는 문서 d 내에서 단어 t의 총 빈도를 f(t,d)라 할 경우, 가장 단순한 tf 산출 방식은 tf(t,d) = f(t,d)로 표현

* 불린 빈도 - tf(t,d) = 1: t가 d에 한 번이라도 나타나면 1, 아니면 0
<br/>
* 로그 스케일 빈도: tf(t,d) = log (f(t,d) + 1);
<br/>
* 증가 빈도: 문서의 길이에 따라 단어의 빈도값 조정

$${tf(t,d) = 0.5 + {  0.5 * f(t,f) \over \max_\{f(w,d): w \in d\}}}$$

<br/>
<br/>
역문서 빈도(IDF)는 한 단어가 문서 집합 전체에서 얼마나 공통적으로 나타나는지를 나타내는 값



$${\mathrm  {idf}}(t,D)=\log {\frac  {|D|}{|\{d\in D:t\in d\}|}}$$

<br/>

* $|D|$: 문서 집합 D의 크기, 또는 전체 문서의 수
* $|\{d\in D:t\in d\}|$:단어 t가 전체 말뭉치 안에 존재하지 않을 경우 이는 분모가 0이 되는 결과를 가져온다. 이를 방지하기 위해 ${\displaystyle 1+|\{d\in D:t\in d\}|} 1+|\{d\in D:t\in d\}|$로 쓰는 것이 일반적이다.
<br/>

$${{\text{idf}(d, t) = \log \dfrac{n}{1 + \text{df}(t)}}}$$

<br/><br/>
## 결과

특정 문서 내에서 단어 빈도가 높을 수록, 그리고 전체 문서들 중 그 단어를 포함한 문서가 적을 수록 TF-IDF값이 높아진다

모든 문서에 흔하게 나타나는 단어를 걸러내는 효과

특정 단어를 포함하는 문서들이 많을 수록 로그 함수 안의 값이 1에 가까워지게 되고(log1 = 0) , 이 경우 IDF값과 TF-IDF값은 0에 가까워지게 된다


{% highlight python3 %}

from sklearn.feature_extraction.text import TfidfVectorizer

corpus = [
    'This is the first document.',
    'This is the second second document.',
    'And the third one.',
    'Is this the first document?',
    'The last document?',    
]


tfidv = TfidfVectorizer().fit(corpus)
print(tfidv.transform(corpus).toarray())

"""
[[0.         0.38947624 0.55775063 0.4629834  0.         0.
  0.         0.32941651 0.         0.4629834 ]
 [0.         0.24151532 0.         0.28709733 0.         0.
  0.85737594 0.20427211 0.         0.28709733]
 [0.55666851 0.         0.         0.         0.         0.55666851
  0.         0.26525553 0.55666851 0.        ]
 [0.         0.38947624 0.55775063 0.4629834  0.         0.
  0.         0.32941651 0.         0.4629834 ]
 [0.         0.45333103 0.         0.         0.80465933 0.
  0.         0.38342448 0.         0.        ]]
"""

{% endhighlight %}

## How to calculate TF-IDF in Scikit learn

{% highlight python3 %}

from sklearn.feature_extraction.text import TfidfVectorizer

corpus = ['This is string',
        'This is another string',
        'TFIDF computation calculation'
    ,'TfIDF is the product of TF and IDF']


tfidv = TfidfVectorizer(norm = None).fit(corpus)
tfidv.transform(corpus).toarray()

print(tfidv.transform(corpus).toarray())
"""
[[0.         0.         0.         0.         0.         1.22314355
  0.         0.         1.51082562 0.         0.         0.
  1.51082562]
 [0.         1.91629073 0.         0.         0.         1.22314355
  0.         0.         1.51082562 0.         0.         0.
  1.51082562]
 [0.         0.         1.91629073 1.91629073 0.         0.
  0.         0.         0.         0.         1.51082562 0.
  0.        ]
 [1.91629073 0.         0.         0.         1.91629073 1.22314355
  1.91629073 1.91629073 0.         1.91629073 1.51082562 1.91629073
  0.        ]]
"""

{% endhighlight %}

* norm = None으로 정규화를 시키지 않았다.


1. tfs are calculated by CountVectorizer's fit_transform()
2. idfs are calculated by TfidfTransformer's fit()
3. tfidfs are calculated by TfidfTransformer's transform()



{% highlight python3 %}

tf = CountVectorizer().fit_transform(corpus)
print(tf.toarray())

"""
[[0 0 0 0 0 1 0 0 1 0 0 0 1]
 [0 1 0 0 0 1 0 0 1 0 0 0 1]
 [0 0 1 1 0 0 0 0 0 0 1 0 0]
 [1 0 0 0 1 1 1 1 0 1 1 1 0]]
"""

"""Transform a count matrix to a normalized tf or tf-idf representation""""
tf_transformer = TfidfTransformer(norm=None).fit(tf)

tf_idf = tf_transformer.transform(tf)
print(tf_idf.toarray())

{% endhighlight %}

<br>

'this' : tf = 1, df = 2 , N = 4

idf = $$\text {idf} = \log {(N + 1 / df + 1)} + 1 = 1.5108256238$$

tf * idf = 1.5108256238
