---
layout: post
title:  "python correlation(상관 관계)"
date:   2018-03-02 08:43:59
author: kim-jaehun
categories: [Android]
tags: Fragment
---



### 싱관관계 (correlation)이란?

> 상관분석(Correlation Analysis)은 확률론과 통계학에서 두 변수간에 어떤 선형적 관계를 갖고 있는 지를 분석하는 방법입니다. 두변수는 서로 독립적인 관계로부터 서로 상관된 관계일 수 있으며 이때 두 변수간의 관계의 강도를 상관관계(Correlation, Correlation coefficient)라 합니다. 상관분석에서는 상관관계의 정도를 나타내는 단위로 모상관계수 ρ를 사용합니다.

> 상관관계의 정도를 파악하는 상관계수(Correlation coefficient)는 두 변수간의 연관된 정도를 나타낼 뿐 인과관계를 설명하는 것은 아닙니다. 두 변수간에 원인과 결과의 인과관계가 있는지에 대한 것은 회귀분석을 통해 인과관계의 방향, 정도와 수학적 모델을 확인해 볼 수 있습니다.



### 상관관계 분석 방법


* 피어슨 상관 계수

> 피어슨 상관 계수(Pearson correlation coefficient 또는 Pearson's r)는 두 변수간의 관련성을 구하기 위해 보편적으로 이용된다.

> r = X와 Y가 함께 변하는 정도 / X와 Y가 각각 변하는 정도

* 결과의 해석
> r 값은 X 와 Y 가 완전히 동일하면 +1, 전혀 다르면 0, 반대방향으로 완전히 동일 하면 –1 을 가진다.



<br>
<table>
  <thead>
    <tr>
      <th>상관 계수</th>
      <th>의미</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>-1.0 ~ -0.7</td>
      <td>강한 음적 선형관계</td>
    </tr>
    <tr>
      <td>-0.7 ~ -0.3</td>
      <td>뚜렷한 음적 선형관계</td>
    </tr>
    <tr>
      <td>-0.3 ~ -0.1</td>
      <td>약한 음적 선형관계</td>
    </tr>
    <tr>
      <td>-0.1 ~ +0.1</td>
      <td>거의 무시될 수 있는 선형관계</td>
    </tr>
    <tr>
      <td>+0.1 ~ +0.3</td>
      <td>약한 양적 선형관계</td>
    </tr>
    <tr>
      <td>+0.3 ~ +0.7</td>
      <td>뚜렷한 양적 선형관계</td>
    </tr>
    <tr>
      <td>+0.7 ~ +1.0</td>
      <td>강한 양적 선형관계</td>
    </tr>
  </tbody>
</table>


<br>
<br>
### 상관관계 (python) 구현
```python
import pandas as pd

lst = [[100.0, 5.0, 2.0, 1.0, 2.0, 8.2, 8.2, 8.2, 8.2, 8.2, 73.0, 221.0, 1.4, 73.0, 924.2, 0.0, 4.5, 399.0, 0.5, 0.023, 68.0, 1.0],
[100.0, 5.0, 2.0, 1.0, 2.0, 8.0, 8.0, 8.0, 8.0, 8.0, 77.0, 206.0, 0.9, 77.0, 924.3, 0.0, 4.5, 399.0, 0.5, 0.023, 68.0, 1.0],
[100.0, 5.0, 2.0, 1.0, 2.0, 8.0, 8.0, 8.0, 8.0, 8.0, 76.0, 219.0, 1.4, 76.0, 924.7, 0.0, 4.5, 399.0, 0.5, 0.023, 68.0, 1.0],
[100.0, 5.0, 2.0, 1.0, 2.0, 7.8, 7.8, 7.8, 7.8, 7.8, 76.0, 219.0, 1.5, 76.0, 924.8, 0.0, 4.5, 399.0, 0.5, 0.023, 68.0, 1.0],
[100.0, 5.0, 2.0, 1.0, 2.0, 7.7, 7.7, 7.7, 7.7, 7.7, 77.0, 221.0, 1.4, 77.0, 924.8, 0.0, 4.5, 399.0, 0.5, 0.023, 68.0, 1.0],
[100.0, 5.0, 2.5, 1.0, 2.0, 7.3, 7.3, 7.3, 7.3, 7.3, 78.0, 214.0, 1.4, 78.0, 925.0, 300.0, 4.5, 399.0, 0.5, 0.023, 68.0, 1.0],
[100.0, 5.0, 2.5, 1.0, 2.0, 7.1, 7.1, 7.1, 7.1, 7.1, 78.0, 213.0, 1.4, 78.0, 925.3, 800.0, 4.5, 399.0, 0.5, 0.023, 68.0, 1.2],
[100.0, 5.0, 2.5, 1.0, 2.0, 7.2, 7.2, 7.2, 7.2, 7.2, 74.0, 218.0, 1.6, 74.0, 925.5, 800.0, 4.5, 399.0, 0.5, 0.023, 68.0, 1.2],
[100.0, 5.0, 2.5, 1.0, 2.0, 7.3, 7.3, 7.3, 7.3, 7.3, 76.0, 210.0, 1.0, 76.0, 926.0, 800.0, 4.5, 399.0, 0.5, 0.023, 65.0, 1.2],
[100.0, 6.0, 2.5, 3.0, 2.0, 8.5, 8.5, 8.5, 8.5, 8.5, 72.0, 190.0, 0.8, 72.0, 926.7, 1200.0, 4.5, 399.0, 0.5, 0.023, 65.0, 1.2],
[120.0, 6.0, 2.5, 3.0, 1.8, 9.6, 9.6, 9.6, 9.6, 9.6, 67.0, 185.0, 0.8, 67.0, 927.2, 1200.0, 4.5, 399.0, 0.5, 0.023, 65.0, 1.2],
[120.0, 6.0, 2.5, 3.0, 1.8, 9.8, 9.8, 9.8, 9.8, 9.8, 63.0, 204.0, 1.0, 63.0, 927.2, 1200.0, 4.5, 401.0, 0.5, 0.023, 65.0, 1.2],
[120.0, 6.0, 2.5, 3.0, 1.8, 9.7, 9.7, 9.7, 9.7, 9.7, 61.0, 208.0, 0.9, 61.0, 927.0, 1600.0, 4.5, 401.0, 0.6, 0.023, 65.0, 1.2],
[120.0, 6.0, 2.5, 3.0, 1.8, 8.1, 8.1, 8.1, 8.1, 8.1, 66.0, 208.0, 1.2, 66.0, 926.7, 1600.0, 4.5, 401.0, 0.6, 0.023, 65.0, 1.2],
[120.0, 6.0, 3.0, 3.0, 1.8, 11.2, 11.2, 11.2, 11.2, 11.2, 61.0, 182.0, 0.6, 61.0, 926.6, 1400.0, 4.5, 401.0, 0.6, 0.023, 65.0, 1.2],
[120.0, 6.0, 3.0, 3.0, 1.8, 11.3, 11.3, 11.3, 11.3, 11.3, 57.0, 204.0, 0.9, 57.0, 926.7, 1400.0, 4.5, 401.0, 0.6, 0.023, 65.0, 1.2],
[120.0, 6.0, 3.0, 3.0, 1.8, 11.0, 11.0, 11.0, 11.0, 11.0, 62.0, 184.0, 0.3, 62.0, 927.3, 1400.0, 4.5, 401.0, 0.6, 0.023, 60.0, 1.2],
[120.0, 6.0, 3.0, 3.0, 1.8, 10.5, 10.5, 10.5, 10.5, 10.5, 68.0, 213.0, 0.5, 68.0, 928.0, 800.0, 4.5, 401.0, 0.6, 0.023, 60.0, 1.2],
[80.0, 6.0, 3.0, 3.0, 1.8, 10.1, 10.1, 10.1, 10.1, 10.1, 76.0, 212.0, 0.8, 76.0, 928.8, 800.0, 4.5, 401.0, 0.6, 0.023, 60.0, 1.2],
[80.0, 7.0, 3.0, 2.0, 2.2, 9.5, 9.5, 9.5, 9.5, 9.5, 78.0, 232.0, 0.7, 78.0, 929.8, 500.0, 4.5, 390.0, 0.6, 0.023, 60.0, 0.8],
[80.0, 7.0, 3.0, 2.0, 2.2, 9.1, 9.1, 9.1, 9.1, 9.1, 82.0, 229.0, 0.6, 82.0, 930.8, 300.0, 4.5, 390.0, 0.6, 0.023, 60.0, 0.8],
[80.0, 7.0, 3.0, 2.0, 2.2, 8.9, 8.9, 8.9, 8.9, 8.9, 83.0, 239.0, 0.5, 83.0, 931.8, 0.0, 4.5, 390.0, 0.6, 0.023, 60.0, 0.8],
[80.0, 7.0, 3.0, 2.0, 2.2, 8.5, 8.5, 8.5, 8.5, 8.5, 85.0, 18.0, 0.5, 85.0, 932.3, 0.0, 4.5, 390.0, 0.6, 0.023, 60.0, 0.8],
[80.0, 7.0, 3.0, 2.0, 2.2, 8.3, 8.3, 8.3, 8.3, 8.3, 83.0, 0.0, 2.2, 83.0, 933.0, 0.0, 4.5, 410.0, 0.6, 0.023, 60.0, 0.8]
]


df = pd.DataFrame(lst)
print(df.corr(method = 'pearson'))

# excel로 저장
df.corr().to_excel("file.xlsx")
# csv로 저장
df.corr().to_csv("file.csv")

```




<br><br>
#### 참고문헌
* https://ko.wikipedia.org/wiki/%EC%83%81%EA%B4%80_%EB%B6%84%EC%84%9D
* http://gomguard.tistory.com/173
