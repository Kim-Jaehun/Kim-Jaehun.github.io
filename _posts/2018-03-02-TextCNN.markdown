---
layout: post
title:  "TextCNN"
date:   2018-03-02 08:43:59
author: kim-jaehun
categories: [Deep Learning]
tags: CNN
---

# TextCNN

[Convolutional Neural Networks for Sentence Classification](http://emnlp2014.org/papers/pdf/EMNLP2014181.pdf) 에서 제안된 방법이다.

<br>

![TextCNN_architecture](https://drive.google.com/uc?id=1V_eDSDjbEFknbFJPkCACO8ZQSdj7O0os)

<br>
<br>

#### CNN을 이용해서 Text Classification 구현
<br>
{% highlight python3 %}
"""Load data"""
x_text, y = data_helpers.load_data_and_labels(FLAGS.positive_data_file, FLAGS.negative_data_file)
{% endhighlight %}

x_text : 데이터 문장<br>
y : [1,0] 긍정, [0,1] 부정

<br>
{% highlight python3 %}
"""Build vocabulary"""
max_document_length = max([len(x.split(" ")) for x in x_text])
vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)
x = np.array(list(vocab_processor.fit_transform(x_text)))
{% endhighlight %}

max_document_length : 가장 긴 문장의 사이즈
* [learn.preprocessing.VocabularyProcessor](http://tflearn.org/data_utils/)의 예제

{% highlight python3 %}
x = ['i like pizza', 'i like pasta']
vocab_processor = learn.preprocessing.VocabularyProcessor(4)
x = np.array(list(vocab_processor.fit_transform(x)))
"""
[[1 2 3 0]
 [1 2 4 0]]
"""
{% endhighlight %}

<br>
{% highlight python3 %}
"""Randomly shuffle data"""
np.random.seed(10)
shuffle_indices = np.random.permutation(np.arange(len(y)))
x_shuffled = x[shuffle_indices]
y_shuffled = y[shuffle_indices]
{% endhighlight %}

데이터를 무작위로 섞는다.

<br>
{% highlight python3 %}
""" Split train/test set"""
dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))
x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]
y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]
{% endhighlight %}

데이터를 학습데이터 , 평가데이터로 나눈다.

<br>
{% highlight python3 %}
""" Split train/test set"""
dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))
x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]
y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]
{% endhighlight %}



### Train
<br>
{% highlight python3 %}
"""Placeholders for input, output and dropout"""
self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name="input_x")
self.input_y = tf.placeholder(tf.float32, [None, num_classes], name="input_y")
self.dropout_keep_prob = tf.placeholder(tf.float32, name="dropout_keep_prob")

"""Embedding layer"""
with tf.device('/cpu:0'), tf.name_scope("embedding"):
  self.W = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0), name="W")
  self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)
  self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)
{% endhighlight %}

<br>
<br>
{% highlight python3 %}
       pooled_outputs = []
       for i, filter_size in enumerate(filter_sizes):
           with tf.name_scope("conv-maxpool-%s" % filter_size):
               # Convolution Layer
               filter_shape = [filter_size, embedding_size, 1, num_filters]
               W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name="W")
               b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name="b")
               conv = tf.nn.conv2d(
                   self.embedded_chars_expanded,
                   W,
                   strides=[1, 1, 1, 1],
                   padding="VALID",
                   name="conv")
               h = tf.nn.relu(tf.nn.bias_add(conv, b), name="relu")
               pooled = tf.nn.max_pool(
                   h,
                   ksize=[1, sequence_length - filter_size + 1, 1, 1],
                   strides=[1, 1, 1, 1],
                   padding='VALID',
                   name="pool")
               pooled_outputs.append(pooled)

       num_filters_total = num_filters * len(filter_sizes)
       self.h_pool = tf.concat(pooled_outputs, 3)
       self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])

{% endhighlight %}
<br>
<br>

{% highlight python3 %}

 with tf.name_scope("dropout"):
     self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)

 with tf.name_scope("output"):
     W = tf.get_variable(
         "W",
         shape=[num_filters_total, num_classes],
         initializer=tf.contrib.layers.xavier_initializer())
     b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name="b")
     l2_loss += tf.nn.l2_loss(W)
     l2_loss += tf.nn.l2_loss(b)
     self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name="scores")
     self.predictions = tf.argmax(self.scores, 1, name="predictions")

 with tf.name_scope("loss"):
     losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)
     self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss

 with tf.name_scope("accuracy"):
     correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))
     self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"), name="accuracy")

{% endhighlight %}






<br><br>
#### 참고문헌


* https://ratsgo.github.io/natural%20language%20processing/2017/03/19/CNN/
* [Convolutional Neural Networks for Sentence Classification](http://emnlp2014.org/papers/pdf/EMNLP2014181.pdf)
* http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/
